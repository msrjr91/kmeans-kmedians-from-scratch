{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc5481f-b715-43c8-8924-6a6187e98151",
   "metadata": {},
   "source": [
    "### CA2 K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8f79d69-8c32-4ab7-9cf0-b5c98278e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b71bb2-68ef-4291-93bb-149b612abd95",
   "metadata": {},
   "source": [
    "Load data into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af9df3ae-a8da-4412-9cb8-96c1307ab261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = 'CA2data/animals'\n",
    "df2 = 'CA2data/countries'\n",
    "df3 = 'CA2data/fruits'\n",
    "df4 = 'CA2data/veggies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d17b1d4-733e-4cc9-bf9f-4b8b67078d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create initial datasets; get unique values to remap into classes later\n",
    "df_animals = pd.read_csv(df1, header = None, delimiter = ' ')\n",
    "animals_unique = [i for i in df_animals[0]]\n",
    "df_countries = pd.read_csv(df2, header = None, delimiter = ' ')\n",
    "countries_unique = [i for i in df_countries[0]]\n",
    "df_fruits = pd.read_csv(df3, header = None, delimiter = ' ')\n",
    "fruits_unique = [i for i in df_fruits[0]]\n",
    "df_veggies = pd.read_csv(df4, header = None, delimiter = ' ')\n",
    "veggies_unique = [i for i in df_veggies[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc8826e8-0c58-4a77-bc86-0ff44171241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the four datasets into one pandas dataframe\n",
    "frames = [df_animals, df_countries, df_fruits, df_veggies]\n",
    "combined_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb4778-190a-41d1-bd5a-7a1873ffcca3",
   "metadata": {},
   "source": [
    "Need to convert column 0 (labels) into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23209d9b-37ea-45a1-b836-5881c4ed49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas dataframe into numpy arrays\n",
    "df = combined_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "206ab69c-c8ef-4dab-afce-8b38b478d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we have labeled data and know the classes, convert categorical data into numeric for accuracy measurement\n",
    "for array in df:\n",
    "    if array[0] in animals_unique:\n",
    "        array[0] = 0\n",
    "    elif array[0] in countries_unique:\n",
    "        array[0] = 1\n",
    "    elif array[0] in fruits_unique:\n",
    "        array[0] = 2\n",
    "    elif array[0] in veggies_unique:\n",
    "        array[0] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fea0c-ef8b-4f03-b83b-88eabdf5494f",
   "metadata": {},
   "source": [
    "Initialize Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9933aaa6-5ad5-43e2-9d5b-0be292fcbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kmeans++ to choose our starting centroid(s)\n",
    "def kmeansplusplus(dataset, k):\n",
    "    # initialize first random point as starting centroid\n",
    "    centroids = dataset[np.random.choice(dataset.shape[0], 1, replace = False)]\n",
    "    # if/when k = 2\n",
    "    if k > 1:\n",
    "        # select second centroid \n",
    "        distance = np.zeros((dataset.shape[0],1))\n",
    "        distance[:, 0] = ((dataset - centroids[0])**2).sum(axis = 1)\n",
    "        new_centroid = np.argmax(distance, axis = 0)\n",
    "        centroids = np.concatenate((centroids, dataset[new_centroid]))\n",
    "    # new centroids if k > 2    \n",
    "    while len(centroids) != k:\n",
    "        for i in range(2,k):\n",
    "            # initialize new distances with zeros \n",
    "            new_distances = np.zeros((dataset.shape[0], i))   \n",
    "            for y in range(i):\n",
    "                # find distances between each point and each centroids we have\n",
    "                new_distances[:, y] = ((dataset - centroids[y])**2).sum(axis = 1)\n",
    "            # sort from lowest to highest distances    \n",
    "            new_distances = np.sort(new_distances)\n",
    "            # keep only the closest distances\n",
    "            new_distances = np.delete(new_distances, np.s_[1:], axis = 1)\n",
    "            # return max or furthest point's index\n",
    "            max_index = np.argmax(new_distances, axis = 0)\n",
    "            # find new centroids using index\n",
    "            centroids = np.concatenate((centroids, dataset[max_index]))\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d6921-d543-445b-bec2-10cd2590b48a",
   "metadata": {},
   "source": [
    "##### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71b99da8-d629-45e4-a4b0-a9f8425de549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(dataset, k, normalisation):\n",
    "    np.random.seed(1)\n",
    "    # shuffle ordering of arrays\n",
    "    np.random.shuffle(dataset)\n",
    "    # remove classification feature\n",
    "    traindata = dataset[:,1:]\n",
    "    # set classes as labels\n",
    "    classes = dataset[:, 0]\n",
    "    #classes = np.array([i[0] for i in dataset]).T\n",
    "    # if l2 is implemented:\n",
    "    if normalisation == 'l2':\n",
    "        # normalise each vector \n",
    "        for item in traindata:\n",
    "            for num in item:\n",
    "                l2 = norm(item)\n",
    "                num = num/l2\n",
    "    else:\n",
    "        pass\n",
    "    # initialize centroids using kmeans++\n",
    "    centroids = kmeansplusplus(traindata, k)\n",
    "    # establish initial clusters membership as zeros\n",
    "    clusters = np.zeros(traindata.shape[0], dtype = int)\n",
    "    # set initial distances as zeros\n",
    "    euclidean = np.zeros((traindata.shape[0], k))\n",
    "    # while loop to update memberships\n",
    "    while True:\n",
    "        # copy initial clusters as the previous iteration's clusters\n",
    "        old_clusters = clusters.copy()\n",
    "        # measure each point's distance to set of centroids to create vector of length k of distances\n",
    "        for i in range(k):\n",
    "            euclidean[:, i] = ((traindata - centroids[i])**2).sum(axis = 1)**0.5 # subtract each subset of centroids from each point in dataset\n",
    "        # update clusters\n",
    "        clusters = np.argmin(euclidean, axis = 1)\n",
    "        # update centroids from new clusters\n",
    "        for i in range(k):\n",
    "            centroids[i, :] = traindata[clusters == i].mean(axis = 0)\n",
    "        # break while loop if no adjustments made to clusters    \n",
    "        if all(clusters == old_clusters): # the new cluster only indicates memberships, not predictions\n",
    "            break                \n",
    "               \n",
    "    # create dictionary to aid with B-Cubed precision, recall, and F1 scores\n",
    "    cluster_all = {} # expecting a dictionary where clusters are key, list of associated class labels as values\n",
    "    for num in range(k):\n",
    "         # create list of the indices to locate corresponding class instance\n",
    "         temp_list = [i for i in range(len(clusters)) if clusters[i] == num]\n",
    "         # list to append class labels\n",
    "         y_list = [classes[p] for p in temp_list]\n",
    "         cluster_all[num] = y_list\n",
    "    \n",
    "    # precision score\n",
    "    precision = []\n",
    "    for values in cluster_all.values():\n",
    "        for item in values:\n",
    "            precision.append(values.count(item)/len(values))\n",
    "    precision_avg = sum(precision)/len(precision)\n",
    "\n",
    "    # recall score\n",
    "    recall = []\n",
    "    for values in cluster_all.values():\n",
    "        for item in values:\n",
    "            recall.append(values.count(item)/(classes == item).sum(axis = 0))\n",
    "            \n",
    "\n",
    "    recall_avg = sum(recall)/len(recall)    \n",
    "\n",
    "    # F Score\n",
    "    f_scores = []\n",
    "    end_index = len(precision)\n",
    "    for i in range(end_index):\n",
    "        f_scores.append((2*precision[i]*recall[i])/(precision[i]+recall[i]))\n",
    "    f_scores_avg = sum(f_scores)/len(f_scores)\n",
    "\n",
    "    return precision_avg, recall_avg, f_scores_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33053f96-adda-4d70-a213-858bf23be178",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### K-Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80e2d76b-9ee5-4224-9830-56b57421f9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kmedians(dataset, k, normalisation):\n",
    "    np.random.seed(1)\n",
    "    # shuffle ordering of arrays\n",
    "    np.random.shuffle(dataset)\n",
    "    # remove classification feature\n",
    "    traindata = dataset[:,1:]\n",
    "    # set classes as labels\n",
    "    classes = dataset[:, 0]\n",
    "    #classes = np.array([i[0] for i in dataset]).T\n",
    "    # if l2 is implemented:\n",
    "    if normalisation == 'l2':\n",
    "        # normalise each vector \n",
    "        for item in traindata:\n",
    "            for num in item:\n",
    "                l2 = norm(item)\n",
    "                num = num/l2\n",
    "    else:\n",
    "        pass\n",
    "    # initialize centroids using kmeans++\n",
    "    centroids = kmeansplusplus(traindata, k)\n",
    "    # establish initial clusters membership as zeros\n",
    "    clusters = np.zeros(traindata.shape[0], dtype = int)\n",
    "    # set initial distances as zeros\n",
    "    manhattan = np.zeros((traindata.shape[0], k))\n",
    "    # while loop to update memberships\n",
    "    while True:\n",
    "        # copy initial clusters as the previous iteration's clusters\n",
    "        old_clusters = clusters.copy()\n",
    "        # measure each point's distance to set of centroids to create vector of length k of distances\n",
    "        for i in range(k):\n",
    "            manhattan[:, i] = (abs(traindata - centroids[i])).sum(axis = 1) # subtract each subset of centroids from each point in dataset\n",
    "        # update clusters\n",
    "        clusters = np.argmin(manhattan, axis = 1)\n",
    "        # update centroids from new clusters\n",
    "        for i in range(k):\n",
    "            manhattan[i, :] = np.median(traindata[clusters == i])\n",
    "        # break while loop if no adjustments made to clusters    \n",
    "        if all(clusters == old_clusters): # the new cluster only indicates memberships, not predictions\n",
    "            break\n",
    "                        \n",
    "    # create dictionary to aid with B-Cubed precision, recall, and F1 scores\n",
    "    cluster_all = {}\n",
    "    for num in range(k):\n",
    "        # create list of the indices to locate corresponding class instance\n",
    "        temp_list = [i for i in range(len(clusters)) if clusters[i] == num]\n",
    "        # list to append class labels\n",
    "        y_list = [classes[p] for p in temp_list]\n",
    "        cluster_all[num] = y_list\n",
    "    \n",
    "    # precision score\n",
    "    precision = []\n",
    "    for values in cluster_all.values():\n",
    "        for item in values:\n",
    "            precision.append(values.count(item)/len(values))\n",
    "    precision_avg = sum(precision)/len(precision)\n",
    "\n",
    "    # recall score\n",
    "    recall = []\n",
    "    for values in cluster_all.values():\n",
    "        for item in values:\n",
    "            recall.append(values.count(item)/(classes == item).sum())\n",
    "    recall_avg = sum(recall)/len(recall)    \n",
    "\n",
    "    # F Score\n",
    "    f_scores = []\n",
    "    end_index = len(precision)\n",
    "    for i in range(end_index):\n",
    "        f_scores.append((2*precision[i]*recall[i])/(precision[i]+recall[i]))\n",
    "    f_scores_avg = sum(f_scores)/len(f_scores)\n",
    "\n",
    "    return precision_avg, recall_avg, f_scores_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32d75069-a3f2-42d2-ae84-ddcda6cc8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(function, k, normalisation):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(k):\n",
    "        p, r, f = function(df, i+1, normalisation)\n",
    "        y.append((p,r,f))\n",
    "        x.append(i+1)\n",
    "    y1 = [i[0] for i in y]\n",
    "    y2 = [i[1] for i in y]\n",
    "    y3 = [i[2] for i in y]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(x, y1, c = 'green', label = 'Precision')\n",
    "    plt.scatter(x, y2, c = 'red', label = 'Recall')\n",
    "    plt.scatter(x, y3, c = 'blue', label = 'F-Score')\n",
    "    plt.title('Overall Scores')\n",
    "    plt.xlabel('k-Clusters')\n",
    "    plt.ylabel('Scoring Range')    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return y   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e4f14b6-e1a1-4744-8eea-95c966ee514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to run for all four results:\n",
    "\n",
    "# results(kmeans, 9, 'none')\n",
    "# results(kmeans, 9, 'l2')\n",
    "# results(kmedians, 9, 'none')\n",
    "# results(kmedians, 9, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6d184-c183-4d33-9240-2c4b908b1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sample.items())[0][1]['activation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
